{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce18b135-94e3-410a-9215-8bf9b3b2db8f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a236ca8-9617-4bdf-8399-d797b71bdf52",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Mixutre-of-Experts - Achieve Massively Scaled, but Efficient, LLM Peformance \n",
    "In this lab we will explore how to build our own, simplified version of a mixture-of-experts (MoE) LLM system. While this method often involves a complex training and transformer configuration, we can see some of the benefits of this approach in a pseudo-MoE that we will build with some open source LLMs. \n",
    "\n",
    "\n",
    "### ![Dolly](https://files.training.databricks.com/images/llm/dolly_small.png) Learning Objectives\n",
    "1. Create your own MoE system using open source LLMs\n",
    "1. Build different gating mechanisms to direct different prompts to appropriate \"expert models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb45d483-bd54-4553-9aa5-0060faeecce3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36bcc104-b47e-46e2-8ff0-ce8e0a52a439",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54242e38-a6ed-49b7-84cf-690ee3d1db87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install textblob==0.17.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "148496ad-4ff4-471c-b26e-afa5fdfe885a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d8ec1ce-1e56-4058-91ff-1ba5862960c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Section 1: An Overview of Mixture-of-Experts (MoE)\n",
    "Mixture-of-Experts (MoE) is a machine learning architecture that incorporates the idea of \"divide and conquer\" to solve complex problems. In this approach, the model is composed of multiple individual models, referred to as \"experts\", each of which specializes in some aspect of the data. The model also includes a \"gating\" function that determines which expert or combination of experts to consult for a given input.\n",
    "\n",
    "The key feature of MoE models is that they can handle a diverse range of data patterns through the different areas of expertise of their component models. Each expert has its own set of parameters and is typically a simpler model than would be necessary to model the entire data set effectively. The gating mechanism then learns to recognize which expert is most likely to provide the best output for a particular input, thereby effectively dividing the problem space among the different experts.\n",
    "\n",
    "In a true MoE model, the experts and the gating function are trained together in an end-to-end manner. This joint training allows the experts to specialize on different parts of the input space and the gating function to learn how to best utilize the experts based on the input. It's a kind of \"cooperative competition\" among the experts, where they compete to contribute to the final output but cooperate in the sense that their combined expertise leads to a better overall model.\n",
    "\n",
    "An illustrative diagram would show the input being fed into the gating function, which then weights the contribution of each expert model to produce the final output. The expert models themselves would be shown as individual networks, each receiving the same input and producing its own output.\n",
    "\n",
    "The main advantage of MoE models is their efficiency in modeling complex functions with fewer parameters than a single large model. Since different experts can share parameters, this reduces the total number of parameters needed. This feature makes MoE models particularly useful in settings where data is diverse and complex, and a single model may struggle to capture all the different patterns present in the data.\n",
    "\n",
    "In this notebook, we will be creating a \"pseudo\" MoE model. This is not a true MoE model because we are not training the experts and the gating function together in an end-to-end manner. Instead, we will be using pre-trained models as our experts and defining our own simple gating function. While this approach does not fully capture the power of a true MoE model, it provides a useful introduction to the concept and allows us to explore how different experts and gating functions can affect the performance of the model. It also provides a foundation for understanding how a true MoE model might be implemented and trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5cce02a-d2cd-4318-8c2b-66c58b3c2af4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Section 2: The Pseudo MoE Model\n",
    "In this section, we'll implement a simplified version of an MoE model. Instead of training the experts and gating function together, we'll use pre-trained transformer models as our experts and a simple rule-based function as our gating function.\n",
    "\n",
    "We'll also look at different types of gating mechanisms - hard gating, soft gating, and top-k gating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ee94e54-c55c-48ea-9f1b-70378ad04056",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "# transformers is a state-of-the-art library for Natural Language Processing tasks, providing a wide range of pre-trained models\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, BertForSequenceClassification, BertTokenizer, T5ForConditionalGeneration, T5Tokenizer\n",
    "# torch.nn.functional provides functions that don't have any parameters, such as activation functions, loss functions etc.\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the GPT2 model and tokenizer\n",
    "# GPT2 is an autoregressive language model that uses transformer blocks and byte-pair encoding\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2-XL\", cache_dir=DA.paths.datasets+\"/models\")\n",
    "# The tokenizer is responsible for turning input data into the format that the model expects\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-XL\", cache_dir=DA.paths.datasets+\"/models\")\n",
    "\n",
    "# Load the BERT model and tokenizer\n",
    "# BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based machine learning technique for natural language processing pre-training\n",
    "bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", cache_dir=DA.paths.datasets+\"/models\")\n",
    "# The tokenizer is responsible for turning input data into the format that the model expects\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", cache_dir=DA.paths.datasets+\"/models\")\n",
    "\n",
    "# Load the T5 model and tokenizer\n",
    "# T5 (Text-to-Text Transfer Transformer) is a transformer model which treats every NLP problem as a text generation task\n",
    "t5 = T5ForConditionalGeneration.from_pretrained(\"t5-base\", cache_dir=DA.paths.datasets+\"/models\")\n",
    "# The tokenizer is responsible for turning input data into the format that the model expects\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\", cache_dir=DA.paths.datasets+\"/models\"+\"/models\")\n",
    "\n",
    "# Define the \"hard gating\" function\n",
    "# This function decides which model to use based on the length of the input\n",
    "def hard_gating_function(input):\n",
    "    if len(input) < 10:\n",
    "        # For inputs less than 10 characters long, use the GPT2 model\n",
    "        return \"gpt2\", gpt2, gpt2_tokenizer\n",
    "    elif len(input) < 100:\n",
    "        # For inputs less than 100 characters long but greater than 10 characters, use the T5 model\n",
    "        return \"t5\" , t5, t5_tokenizer\n",
    "    else:\n",
    "        # For inputs greater than 100 characters, use the BERT model\n",
    "        return \"bert\", bert, bert_tokenizer\n",
    "\n",
    "# Define the \"soft gating\" function\n",
    "# This function assigns a weight to each model based on the length of the input, and all models are used to a certain extent to generate the output\n",
    "def soft_gating_function(input):\n",
    "    # The weights for each model are calculated using the softmax function, which outputs a probability distribution\n",
    "    weights = F.softmax(torch.tensor([len(input), 100 - len(input), len(input)], dtype=torch.float), dim=0)\n",
    "    # The weights for each model are returned along with the models and their tokenizers\n",
    "    return {\"gpt2\": (gpt2, gpt2_tokenizer, weights[0]),\n",
    "            \"bert\": (bert, bert_tokenizer, weights[1]),\n",
    "            \"t5\": (t5, t5_tokenizer, weights[2])}\n",
    "\n",
    "# Define the pseudo MoE model\n",
    "# This function uses the gating function to decide which model(s) to use for a given input\n",
    "def pseudo_moe_model(input, gating_function):\n",
    "    if gating_function == \"hard\":\n",
    "        # If the hard gating function is used, only one model is used for a given input\n",
    "        model_name, model, tokenizer = hard_gating_function(input)\n",
    "        inputs = tokenizer(input, return_tensors=\"pt\")\n",
    "        if model_name == \"t5\":\n",
    "            # For T5, create a decoder input sequence consisting of only the <BOS> token\n",
    "            decoder_inputs = tokenizer([\"<pad>\"], return_tensors=\"pt\")[\"input_ids\"]\n",
    "            outputs = model(**inputs, decoder_input_ids=decoder_inputs)\n",
    "        else:\n",
    "            outputs = model(**inputs)\n",
    "        # The output of the model is decoded into a string\n",
    "        decoded_output = tokenizer.decode(outputs.logits[0].argmax(-1).tolist())\n",
    "        # The name of the model used and the decoded output are returned\n",
    "        return model_name, decoded_output\n",
    "    else:  # soft gating\n",
    "        # If the soft gating function is used, all models are used to a certain extent to generate the output\n",
    "        models = soft_gating_function(input)\n",
    "        outputs = []\n",
    "        for model_name, (model, tokenizer, weight) in models.items():\n",
    "            inputs = tokenizer(input, return_tensors=\"pt\")\n",
    "            if model_name == \"t5\":\n",
    "                # For T5, create a decoder input sequence consisting of only the <BOS> token\n",
    "                decoder_inputs = tokenizer([\"<pad>\"], return_tensors=\"pt\")[\"input_ids\"]\n",
    "                output = model(**inputs, decoder_input_ids=decoder_inputs)\n",
    "            else:\n",
    "                output = model(**inputs)\n",
    "            # The output of each model is multiplied by its weight\n",
    "            outputs.append((model_name, output.logits * weight))\n",
    "        # The outputs of all models are added together to generate the final output\n",
    "        decoded_outputs = [(model_name, tokenizer.decode(output[0].argmax(-1).tolist())) for model_name, output in outputs]\n",
    "        # The decoded outputs are returned\n",
    "        return decoded_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7285e846-e594-4665-911d-960d49fcc880",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test the hard gating function\n",
    "example_1 = \"Translate to german: This is a short input.\"\n",
    "output = pseudo_moe_model(example_1, gating_function=\"hard\")\n",
    "print(\"Hard gating output:\", output)\n",
    "\n",
    "# Test the soft gating function\n",
    "example_2 = \"This is a longer input. We're adding more text here to make sure it's longer than 50 characters but shorter than 100 characters.\"\n",
    "output = pseudo_moe_model(example_2, gating_function=\"soft\")\n",
    "print(\"Soft gating output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc959d2f-c161-40df-b027-e9cc3396b076",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Section 3: Your Turn\n",
    "Now it's your turn to experiment with the pseudo MoE model. Here are some exercises you can try:\n",
    "\n",
    "Implement a new gating function: Instead of just using the length of the input, can you use a basic sentiment analysis to determine which model to use? You can use the textblob library for the sentiment analysis.\n",
    "\n",
    "Add a new expert: Can you add a new expert to the pseudo MoE model? Try using the distilbert model, which is a smaller, faster, cheaper version of BERT.\n",
    "\n",
    "Test the updated pseudo MoE model: Once you've made your updates, test the pseudo MoE model with example inputs of different sentiment. What do you notice about the performance of the different experts (models) for different input text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "336b1b57-50ba-49a1-aa9c-691908b74439",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1. Implement a new gating function\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "def sentiment_based_gating_function(input):\n",
    "    <FILL_IN>\n",
    "\n",
    "# 2. Add a new expert\n",
    "\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "\n",
    "distilbert = <FILL_IN>\n",
    "distilbert_tokenizer = <FILL_IN>\n",
    "\n",
    "# Update the gating function to include the new expert\n",
    "\n",
    "def updated_gating_function(input):\n",
    "    <FILL_IN>\n",
    "\n",
    "# 3. Test the updated pseudo MoE model\n",
    "\n",
    "test_input = \"<FILL_IN>\"\n",
    "print(pseudo_moe_model(test_input, gating_function='hard'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3064d50a-97cb-44d7-9f22-c0050e50b450",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion3_1(distilbert, distilbert_tokenizer, test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c75736a8-ec76-4ffc-92db-a766d2cf06bf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "LLM 03L - Deployment of LLMs Lab",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
